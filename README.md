# pyspark_optimization
I have taken a big csv file from kaggle for this project, It's size is around 750 MB. It gives error when I try to load the data in pandas. I have performed certain step using pyspark and it takes 45seconds to process the data. It is a marathon dataset.